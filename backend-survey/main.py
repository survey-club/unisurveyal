from fastapi import FastAPI, Depends, HTTPException, status, Header, Request
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy import create_engine, and_
from sqlalchemy.orm import sessionmaker, Session
from typing import Optional, List
import redis
import os
import httpx
from datetime import datetime

from models import Base, Survey, UserSurvey, SurveyStatus as DBSurveyStatus, UserActivity
from schemas import (
    SurveyCreate, SurveyResponse, UserSurveyCreate, UserSurveyResponse,
    InterestFieldsRequest, RecommendationResponse
)
from scraper import ArxivScraper
from keyword_extractor import KeywordExtractor
from recommender import SurveyRecommender

# í™˜ê²½ ë³€ìˆ˜
DATABASE_URL = os.getenv("DATABASE_URL")
REDIS_URL = os.getenv("REDIS_URL")
AUTH_SERVICE_URL = os.getenv("AUTH_SERVICE_URL", "http://backend-auth:8000")

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Redis í´ë¼ì´ì–¸íŠ¸
redis_client = redis.from_url(REDIS_URL, decode_responses=True)

# ArXiv Scraper
scraper = ArxivScraper()

# í‚¤ì›Œë“œ ì¶”ì¶œê¸°
keyword_extractor = KeywordExtractor()

# ì¶”ì²œ ì‹œìŠ¤í…œ
recommender = SurveyRecommender()

# FastAPI ì•±
app = FastAPI(title="Survey Service", version="2.0.0")

@app.on_event("startup")
def startup_event():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    import time
    max_retries = 10
    retry_interval = 3

    for attempt in range(max_retries):
        try:
            Base.metadata.create_all(bind=engine)
            print("Database tables created successfully")
            break
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"Failed to create tables (attempt {attempt + 1}/{max_retries}): {e}")
                print(f"Retrying in {retry_interval} seconds...")
                time.sleep(retry_interval)
            else:
                print(f"Failed to create tables after {max_retries} attempts: {e}")
                raise

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë¡œê¹… ë¯¸ë“¤ì›¨ì–´
@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = datetime.now()

    # ìš”ì²­ ë¡œê¹…
    print(f"\n{'='*80}")
    print(f"ğŸ“Š [SURVEY] {request.method} {request.url.path}")
    print(f"â° Time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

    # Query params ë¡œê¹…
    if request.query_params:
        print(f"ğŸ” Query: {dict(request.query_params)}")

    # Authorization í—¤ë” ì²´í¬
    auth_header = request.headers.get("authorization")
    if auth_header:
        print(f"ğŸ‘¤ Authenticated Request")

    response = await call_next(request)

    # ì‘ë‹µ ë¡œê¹…
    duration = (datetime.now() - start_time).total_seconds()
    status_emoji = "âœ…" if response.status_code < 400 else "âŒ"
    print(f"{status_emoji} Status: {response.status_code} | Duration: {duration:.3f}s")
    print(f"{'='*80}\n")

    return response

# ì˜ì¡´ì„±
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

async def verify_token(authorization: Optional[str] = Header(None)):
    """Auth ì„œë¹„ìŠ¤ë¥¼ í†µí•´ í† í° ê²€ì¦"""
    if not authorization:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Not authenticated"
        )

    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{AUTH_SERVICE_URL}/verify",
                headers={"Authorization": authorization}
            )

            if response.status_code != 200:
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Invalid token"
                )

            return response.json()
    except httpx.RequestError:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Auth service unavailable"
        )

# ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
def read_root():
    return {"service": "Survey Service", "status": "running", "version": "2.0.0"}

@app.get("/surveys/user", response_model=List[UserSurveyResponse])
async def get_user_surveys(
    status_filter: Optional[str] = None,
    user_data: dict = Depends(verify_token),
    db: Session = Depends(get_db)
):
    """ì‚¬ìš©ìì˜ Survey ëª©ë¡ ì¡°íšŒ"""
    user_id = user_data["user_id"]

    query = db.query(UserSurvey).filter(UserSurvey.user_id == user_id)

    if status_filter:
        query = query.filter(UserSurvey.status == status_filter)

    user_surveys = query.all()

    # Survey ì •ë³´ í¬í•¨
    result = []
    for us in user_surveys:
        survey = db.query(Survey).filter(Survey.id == us.survey_id).first()
        us_dict = {
            "id": us.id,
            "user_id": us.user_id,
            "survey_id": us.survey_id,
            "status": us.status.value if hasattr(us.status, 'value') else us.status,
            "is_starred": us.is_starred,
            "added_at": us.added_at,
            "completed_at": us.completed_at,
            "survey": survey
        }
        result.append(us_dict)

    return result

@app.get("/surveys/{survey_id}")
async def get_survey(
    survey_id: int,
    increase_view: bool = True,
    user_data: dict = Depends(verify_token),
    db: Session = Depends(get_db)
):
    """íŠ¹ì • Survey ìƒì„¸ ì •ë³´ (ì¡°íšŒìˆ˜ëŠ” ì‚¬ìš©ìê°€ ë³´ê´€í•œ ë…¼ë¬¸ì¼ ë•Œë§Œ ì¦ê°€)"""
    survey = db.query(Survey).filter(Survey.id == survey_id).first()

    if not survey:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Survey not found"
        )

    # ì‚¬ìš©ìì˜ UserSurvey ì •ë³´ í™•ì¸
    user_id = user_data["user_id"]
    user_survey = db.query(UserSurvey).filter(
        and_(
            UserSurvey.user_id == user_id,
            UserSurvey.survey_id == survey_id
        )
    ).first()

    # ì¡°íšŒìˆ˜ ì¦ê°€ - ë³´ê´€í•¨ì— ì¶”ê°€ëœ ë…¼ë¬¸ì´ê³  increase_viewê°€ Trueì¼ ë•Œë§Œ
    if user_survey and increase_view:
        survey.view_count = (survey.view_count or 0) + 1
        db.commit()
        db.refresh(survey)

    return {
        "survey": survey,
        "user_survey": {
            "id": user_survey.id,
            "user_id": user_survey.user_id,
            "survey_id": user_survey.survey_id,
            "status": user_survey.status.value if hasattr(user_survey.status, 'value') else user_survey.status,
            "is_starred": user_survey.is_starred,
            "added_at": user_survey.added_at,
            "completed_at": user_survey.completed_at
        } if user_survey else None
    }

@app.post("/surveys/add", response_model=UserSurveyResponse)
async def add_survey_to_user(
    user_survey: UserSurveyCreate,
    user_data: dict = Depends(verify_token),
    db: Session = Depends(get_db)
):
    """ì‚¬ìš©ìì—ê²Œ Survey ì¶”ê°€ (ë³´ê´€í•¨ì— ì €ì¥)"""
    user_id = user_data["user_id"]

    # ì´ë¯¸ ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸
    existing = db.query(UserSurvey).filter(
        and_(
            UserSurvey.user_id == user_id,
            UserSurvey.survey_id == user_survey.survey_id
        )
    ).first()

    if existing:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Survey already added"
        )

    # Survey ì¡´ì¬ í™•ì¸
    survey = db.query(Survey).filter(Survey.id == user_survey.survey_id).first()
    if not survey:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Survey not found"
        )

    # UserSurvey ìƒì„±
    new_user_survey = UserSurvey(
        user_id=user_id,
        survey_id=user_survey.survey_id,
        status=user_survey.status
    )

    db.add(new_user_survey)
    db.commit()
    db.refresh(new_user_survey)

    return {
        "id": new_user_survey.id,
        "user_id": new_user_survey.user_id,
        "survey_id": new_user_survey.survey_id,
        "status": new_user_survey.status.value if hasattr(new_user_survey.status, 'value') else new_user_survey.status,
        "is_starred": new_user_survey.is_starred,
        "added_at": new_user_survey.added_at,
        "completed_at": new_user_survey.completed_at,
        "survey": survey
    }

@app.put("/surveys/{user_survey_id}/status")
async def update_survey_status(
    user_survey_id: int,
    new_status: str,
    user_data: dict = Depends(verify_token),
    db: Session = Depends(get_db)
):
    """Survey ìƒíƒœ ì—…ë°ì´íŠ¸ (reading, completed ë“±)"""
    user_id = user_data["user_id"]

    user_survey = db.query(UserSurvey).filter(
        and_(
            UserSurvey.id == user_survey_id,
            UserSurvey.user_id == user_id
        )
    ).first()

    if not user_survey:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User survey not found"
        )

    user_survey.status = DBSurveyStatus[new_status]

    if new_status == "completed":
        from datetime import datetime, date
        user_survey.completed_at = datetime.utcnow()

        # ìŠ¤íŠ¸ë¦­ ê¸°ë¡ (completed ìƒíƒœë¡œ ë³€ê²½ë  ë•Œë§Œ)
        today = date.today()
        activity = db.query(UserActivity).filter(
            and_(
                UserActivity.user_id == user_id,
                UserActivity.activity_date == today
            )
        ).first()

        if activity:
            activity.survey_views += 1
        else:
            activity = UserActivity(
                user_id=user_id,
                activity_date=today,
                survey_views=1
            )
            db.add(activity)

    db.commit()

    return {"message": "Status updated successfully"}

@app.put("/surveys/{user_survey_id}/star")
async def toggle_survey_star(
    user_survey_id: int,
    user_data: dict = Depends(verify_token),
    db: Session = Depends(get_db)
):
    """Survey ì¦ê²¨ì°¾ê¸° í† ê¸€"""
    user_id = user_data["user_id"]

    user_survey = db.query(UserSurvey).filter(
        and_(
            UserSurvey.id == user_survey_id,
            UserSurvey.user_id == user_id
        )
    ).first()

    if not user_survey:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User survey not found"
        )

    # ìŠ¤íƒ€ ìƒíƒœ í† ê¸€
    user_survey.is_starred = not user_survey.is_starred
    db.commit()

    return {
        "message": "Star toggled successfully",
        "is_starred": user_survey.is_starred
    }

@app.delete("/surveys/{survey_id}")
async def delete_user_survey(
    survey_id: int,
    user_data: dict = Depends(verify_token),
    db: Session = Depends(get_db)
):
    """ì‚¬ìš©ìì˜ ë³´ê´€í•¨ì—ì„œ ë…¼ë¬¸ ì‚­ì œ"""
    user_id = user_data["user_id"]

    # survey_idë¡œ user_survey ì°¾ê¸°
    user_survey = db.query(UserSurvey).filter(
        and_(
            UserSurvey.survey_id == survey_id,
            UserSurvey.user_id == user_id
        )
    ).first()

    if not user_survey:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Survey not found in user's collection"
        )

    db.delete(user_survey)
    db.commit()

    return {"message": "Survey removed successfully"}

@app.post("/recommend/initial", response_model=List[SurveyResponse])
async def initial_recommend(
    request: InterestFieldsRequest,
    user_data: dict = Depends(verify_token),
    db: Session = Depends(get_db)
):
    """
    ì´ˆê¸° ì¶”ì²œ: ì‚¬ìš©ìê°€ ì„ íƒí•œ ê´€ì‹¬ ë¶„ì•¼ ê¸°ë°˜ ë…¼ë¬¸ ì¶”ì²œ
    íšŒì›ê°€ì… í›„ ë˜ëŠ” ì½ì€ ë…¼ë¬¸ì´ 5ê°œ ë¯¸ë§Œì¼ ë•Œ ì‚¬ìš©
    """
    print(f"Received request with fields: {request.fields}")

    if not request.fields or len(request.fields) == 0:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="At least one interest field is required"
        )

    # ArXivì—ì„œ ê´€ì‹¬ ë¶„ì•¼ ë…¼ë¬¸ ê²€ìƒ‰
    arxiv_results = scraper.search_ai_ml_surveys(request.fields, max_results=500)

    # DBì— ì €ì¥ ë° ë°˜í™˜
    survey_responses = []
    for result in arxiv_results:
        # ì´ë¯¸ DBì— ìˆëŠ”ì§€ í™•ì¸
        existing_survey = db.query(Survey).filter(
            Survey.arxiv_id == result["arxiv_id"]
        ).first()

        if existing_survey:
            survey_responses.append(existing_survey)
        else:
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = keyword_extractor.extract_from_title_and_abstract(
                result["title"],
                result["abstract"],
                top_n=3
            )
            keywords_str = ", ".join(keywords)

            # ì½ê¸° ì‹œê°„ ì¶”ì •
            reading_times = scraper.estimate_reading_time(result["abstract"])

            # ìƒˆ Survey ìƒì„±
            new_survey = Survey(
                arxiv_id=result["arxiv_id"],
                title=result["title"],
                abstract=result["abstract"],
                keywords=keywords_str,
                authors=result["authors"],
                published_date=result["published_date"],
                pdf_url=result["pdf_url"],
                categories=result["categories"],
                estimated_reading_time_beginner=reading_times["beginner"],
                estimated_reading_time_intermediate=reading_times["intermediate"],
                estimated_reading_time_advanced=reading_times["advanced"],
                tags=", ".join(request.fields),
                view_count=0
            )

            db.add(new_survey)
            db.commit()
            db.refresh(new_survey)

            survey_responses.append(new_survey)

    return survey_responses

@app.post("/recommend/personalized", response_model=List[RecommendationResponse])
async def personalized_recommend(
    user_data: dict = Depends(verify_token),
    db: Session = Depends(get_db),
    top_n: int = 500
):
    """
    ê°œì¸í™” ì¶”ì²œ: TF-IDF + Cosine Similarity ê¸°ë°˜
    1. ArXivì—ì„œ ML/DL Survey ë…¼ë¬¸ 500ê°œ ê°€ì ¸ì˜¤ê¸°
    2. ì‚¬ìš©ìê°€ ì™„ë£Œí•œ ë…¼ë¬¸ê³¼ ìœ ì‚¬ë„ ê³„ì‚°
    3. ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
    """
    user_id = user_data["user_id"]
    username = user_data.get("username", "Unknown")
    print(f"âœ¨ User '{username}' requesting personalized recommendations (top {top_n})")

    # ì‚¬ìš©ìê°€ ì½ì€ ë…¼ë¬¸ ì¡°íšŒ (completed ìƒíƒœ)
    user_surveys = db.query(UserSurvey).filter(
        and_(
            UserSurvey.user_id == user_id,
            UserSurvey.status == DBSurveyStatus.completed
        )
    ).all()

    print(f"ğŸ“š User has completed {len(user_surveys)} papers")

    if len(user_surveys) < 5:
        print(f"âŒ Not enough papers (minimum 5 required)")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="ìµœì†Œ 5ê°œ ì´ìƒì˜ ë…¼ë¬¸ì„ ì½ì–´ì•¼ ê°œì¸í™” ì¶”ì²œì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )

    # 1. ArXivì—ì„œ ML/DL Survey ë…¼ë¬¸ 500ê°œ ê²€ìƒ‰
    print(f"ğŸ” Fetching 500 ML/DL survey papers from ArXiv...")
    scraper = ArxivScraper()
    arxiv_papers = scraper.search_ml_surveys_for_recommendation(max_results=500)
    print(f"âœ… Found {len(arxiv_papers)} papers from ArXiv")

    # 2. DBì— ì—†ëŠ” ë…¼ë¬¸ ì €ì¥ (í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ)
    saved_surveys = []
    for paper_data in arxiv_papers:
        existing = db.query(Survey).filter(Survey.arxiv_id == paper_data["arxiv_id"]).first()
        if not existing:
            # í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ
            keywords_list = keyword_extractor.extract_from_title_and_abstract(
                paper_data["title"],
                paper_data["abstract"],
                top_n=5
            )
            # ë¦¬ìŠ¤íŠ¸ë¥¼ ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë³€í™˜
            keywords_str = ", ".join(keywords_list) if keywords_list else None

            new_survey = Survey(
                arxiv_id=paper_data["arxiv_id"],
                title=paper_data["title"],
                abstract=paper_data["abstract"],
                authors=paper_data["authors"],
                published_date=paper_data["published_date"],
                pdf_url=paper_data["pdf_url"],
                categories=paper_data["categories"],
                keywords=keywords_str
            )
            db.add(new_survey)
            db.commit()
            db.refresh(new_survey)
            saved_surveys.append(new_survey)
        else:
            # ê¸°ì¡´ ë…¼ë¬¸ì— í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì¶”ì¶œ
            if not existing.keywords:
                keywords_list = keyword_extractor.extract_from_title_and_abstract(
                    existing.title,
                    existing.abstract,
                    top_n=5
                )
                existing.keywords = ", ".join(keywords_list) if keywords_list else None
                db.commit()
            saved_surveys.append(existing)

    print(f"ğŸ’¾ Saved/Retrieved {len(saved_surveys)} papers")

    # 3. ì½ì€ ë…¼ë¬¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    read_paper_ids = [us.survey_id for us in user_surveys]
    read_papers = db.query(Survey).filter(Survey.id.in_(read_paper_ids)).all()

    print(f"ğŸ¤– Computing TF-IDF + Cosine Similarity...")

    # ë…¼ë¬¸ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    read_papers_dict = [
        {
            'id': p.id,
            'title': p.title,
            'abstract': p.abstract,
            'keywords': p.keywords or ''
        }
        for p in read_papers
    ]

    candidate_papers_dict = [
        {
            'id': p.id,
            'title': p.title,
            'abstract': p.abstract,
            'keywords': p.keywords or ''
        }
        for p in saved_surveys
    ]

    # TF-IDF + Cosine Similarity ì¶”ì²œ
    recommendations = recommender.recommend(
        user_read_papers=read_papers_dict,
        all_papers=candidate_papers_dict,
        top_n=top_n
    )

    print(f"âœ… Generated {len(recommendations)} recommendations")

    # ê²°ê³¼ ìƒì„±
    result = []
    for paper_id, similarity_score in recommendations:
        survey = db.query(Survey).filter(Survey.id == paper_id).first()
        if survey:
            result.append({
                "survey": survey,
                "similarity_score": round(float(similarity_score) * 100, 2)  # í¼ì„¼íŠ¸ë¡œ ë³€í™˜
            })

    return result

@app.get("/user/stats")
async def get_user_stats(
    user_data: dict = Depends(verify_token),
    db: Session = Depends(get_db)
):
    """ì‚¬ìš©ì í†µê³„ ì •ë³´ (ë³´ê´€ ì¤‘ì¸ ë…¼ë¬¸ ìˆ˜, ì¶”ì²œë°›ì€ ë…¼ë¬¸ ìˆ˜ ë“±)"""
    user_id = user_data["user_id"]

    # ë³´ê´€ ì¤‘ì¸ ë…¼ë¬¸ ìˆ˜
    saved_count = db.query(UserSurvey).filter(
        UserSurvey.user_id == user_id
    ).count()

    # ì™„ë£Œí•œ ë…¼ë¬¸ ìˆ˜
    completed_count = db.query(UserSurvey).filter(
        and_(
            UserSurvey.user_id == user_id,
            UserSurvey.status == DBSurveyStatus.completed
        )
    ).count()

    # ì¶”ì²œë°›ì€ ë…¼ë¬¸ ìˆ˜ (recommended ìƒíƒœ)
    recommended_count = db.query(UserSurvey).filter(
        and_(
            UserSurvey.user_id == user_id,
            UserSurvey.status == DBSurveyStatus.recommended
        )
    ).count()

    return {
        "saved_surveys": saved_count,
        "completed_surveys": completed_count,
        "recommended_surveys": recommended_count,
        "can_use_personalized": completed_count >= 5
    }


@app.get("/activity/streak")
async def get_user_streak(
    user_data: dict = Depends(verify_token),
    db: Session = Depends(get_db)
):
    """ì‚¬ìš©ìì˜ ìŠ¤íŠ¸ë¦­ ë°ì´í„° ì¡°íšŒ (ìµœê·¼ 365ì¼)"""
    from datetime import date, timedelta

    user_id = user_data["user_id"]
    today = date.today()
    one_year_ago = today - timedelta(days=365)

    # ìµœê·¼ 365ì¼ê°„ì˜ í™œë™ ê¸°ë¡ ì¡°íšŒ
    activities = db.query(UserActivity).filter(
        and_(
            UserActivity.user_id == user_id,
            UserActivity.activity_date >= one_year_ago,
            UserActivity.activity_date <= today
        )
    ).all()

    # ë‚ ì§œë³„ í™œë™ ë§µ ìƒì„±
    activity_map = {}
    for activity in activities:
        activity_map[activity.activity_date.isoformat()] = activity.survey_views

    # 365ì¼ì¹˜ ë°ì´í„° ìƒì„± (ì—†ëŠ” ë‚ ì§œëŠ” 0)
    streak_data = []
    for i in range(365):
        check_date = today - timedelta(days=364 - i)
        date_str = check_date.isoformat()
        streak_data.append({
            "date": date_str,
            "count": activity_map.get(date_str, 0)
        })

    return {
        "streak_data": streak_data,
        "total_days_active": len(activities)
    }

@app.get("/search", response_model=List[SurveyResponse])
async def search_surveys(
    q: str,
    max_results: int = 500,
    user_data: dict = Depends(verify_token),
    db: Session = Depends(get_db)
):
    """
    í‚¤ì›Œë“œ ê¸°ë°˜ ë…¼ë¬¸ ê²€ìƒ‰

    Args:
        q: ê²€ìƒ‰ í‚¤ì›Œë“œ
        max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
    """
    username = user_data.get("username", "Unknown")
    print(f"ğŸ” User '{username}' searching: '{q}' (max: {max_results})")

    if not q or len(q.strip()) == 0:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Search query is required"
        )

    # ArXivì—ì„œ ê²€ìƒ‰
    print(f"ğŸ“¡ Fetching from ArXiv...")
    arxiv_results = scraper.search_surveys(q, max_results=max_results)
    print(f"âœ… Found {len(arxiv_results)} papers from ArXiv")

    # DBì— ì €ì¥ ë° ë°˜í™˜
    survey_responses = []
    for result in arxiv_results:
        # ì´ë¯¸ DBì— ìˆëŠ”ì§€ í™•ì¸
        existing_survey = db.query(Survey).filter(
            Survey.arxiv_id == result["arxiv_id"]
        ).first()

        if existing_survey:
            survey_responses.append(existing_survey)
        else:
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = keyword_extractor.extract_from_title_and_abstract(
                result["title"],
                result["abstract"],
                top_n=3
            )
            keywords_str = ", ".join(keywords)

            # ì½ê¸° ì‹œê°„ ì¶”ì •
            reading_times = scraper.estimate_reading_time(result["abstract"])

            # ìƒˆ Survey ìƒì„±
            new_survey = Survey(
                arxiv_id=result["arxiv_id"],
                title=result["title"],
                abstract=result["abstract"],
                keywords=keywords_str,
                authors=result["authors"],
                published_date=result["published_date"],
                pdf_url=result["pdf_url"],
                categories=result["categories"],
                estimated_reading_time_beginner=reading_times["beginner"],
                estimated_reading_time_intermediate=reading_times["intermediate"],
                estimated_reading_time_advanced=reading_times["advanced"],
                tags=q,
                view_count=0
            )

            db.add(new_survey)
            db.commit()
            db.refresh(new_survey)

            survey_responses.append(new_survey)

    return survey_responses
